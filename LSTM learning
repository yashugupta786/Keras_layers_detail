Keras python LSTM (sequential modeling)
https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks
https://www.youtube.com/watch?v=8h8Z_pKyifM&list=PL1w8k37X_6L9s6pcqz4rAIEYZtF6zKjUE&index=5
https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9
keras embbeding layer-vector represatation of words
create embedings on the fly- it take 3 arguments vocab size,embeding dimension, max len

https://github.com/yashugupta786/Keras_layers_detail/blob/master/keras_embeddings_layer.ipynb
we get embbeding from keras embeding layer on the fly 
it will generate embedings for all the documents with dimension of shape (maxlenth* embedingdimension)

Input and output shape of keras lstm
input_shape takes 3 parameters  
batch_size- optional
timesteps - no of words suppose
features we have (rows)

model.add(lstm(64)
if we have input shape is (10,4,8)
which means
10 records
4 is max len
8 is embeding size here

now after applying the lstm 
10*64

https://www.youtube.com/watch?v=CcGf_Uo7NMw&list=PL1w8k37X_6L9s6pcqz4rAIEYZtF6zKjUE&index=6
